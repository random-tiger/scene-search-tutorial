{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Searching for specific scenes in a movie can be time-consuming. By combining video processing, AI-generated captions, and embeddings, we can create a powerful tool that allows users to find scenes based on textual descriptions. This tool extracts key frames from a video, generates captions for each frame, computes embeddings, and allows users to search through the scenes using natural language queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Required Libraries\n",
    "Install the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai\n",
    "%pip install opencv-python\n",
    "%pip install scikit-learn\n",
    "%pip install scenedetect\n",
    "%pip install PySceneDetect\n",
    "%pip install boto3\n",
    "%pip install pandas\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scenedetect import VideoManager, SceneManager\n",
    "from scenedetect.detectors import ContentDetector\n",
    "from IPython.display import Image, display\n",
    "import concurrent.futures\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing OpenAI Client\n",
    "Initialize the OpenAI client using your API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "# Initializing OpenAI client\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS S3 Configuration\n",
    "Set up your AWS S3 client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Credentials\n",
    "aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "# AWS S3 Configuration\n",
    "s3_bucket_name = 'your-bucket-name'\n",
    "s3_region = 'your-aws-region'\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3',\n",
    "                  region_name=s3_region,\n",
    "                  aws_access_key_id=aws_access_key_id,\n",
    "                  aws_secret_access_key=aws_secret_access_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace 'your-s3-bucket-name' and 'your-s3-region' with your AWS S3 bucket name and region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Video from S3\n",
    "We'll start by downloading the video file from AWS S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download video from S3\n",
    "def download_video_from_s3(s3_bucket, s3_key, local_path):\n",
    "    s3.download_file(s3_bucket, s3_key, local_path)\n",
    "    print(f\"Downloaded {s3_key} from S3 to {local_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the function with the appropriate parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video S3 key (path within the bucket)\n",
    "s3_video_key = 'movie.mp4'  # Update if your video is stored under a different key\n",
    "\n",
    "# Local path to save the video\n",
    "video_local_path = 'movie.mp4'  # Save in the current working directory\n",
    "\n",
    "# Download the video from S3\n",
    "download_video_from_s3(s3_bucket_name, s3_video_key, video_local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Key Frames from Video\n",
    "Next, we'll extract key frames from the video using PySceneDetect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract key frames from video\n",
    "def extract_key_frames(video_path, output_dir):\n",
    "    import os\n",
    "    import cv2\n",
    "    from scenedetect import VideoManager, SceneManager\n",
    "    from scenedetect.detectors import ContentDetector\n",
    "\n",
    "    # Create a VideoManager object\n",
    "    video_manager = VideoManager([video_path])\n",
    "    # Create a SceneManager object\n",
    "    scene_manager = SceneManager()\n",
    "    # Add ContentDetector algorithm (detects fast cuts)\n",
    "    scene_manager.add_detector(ContentDetector(threshold=30.0))\n",
    "\n",
    "    # Start the video manager and perform scene detection\n",
    "    try:\n",
    "        video_manager.start()\n",
    "        scene_manager.detect_scenes(frame_source=video_manager)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during scene detection: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Get list of detected scenes\n",
    "    scene_list = scene_manager.get_scene_list()\n",
    "    if not scene_list:\n",
    "        print(\"No scenes were detected in the video.\")\n",
    "        return []\n",
    "    else:\n",
    "        print(f\"Detected {len(scene_list)} scenes in video.\")\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Open video file using OpenCV\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Failed to open video file {video_path}\")\n",
    "        return []\n",
    "\n",
    "    # Save the first frame of each scene\n",
    "    frame_list = []\n",
    "    total_scenes = len(scene_list)\n",
    "    for i, scene in enumerate(scene_list):\n",
    "        print(f\"Processing scene {i + 1}/{total_scenes}\")\n",
    "        # Access the start timecode of the scene\n",
    "        start_timecode = scene[0]\n",
    "        # Get the frame number\n",
    "        start_frame = start_timecode.get_frames()\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame_filename = os.path.join(output_dir, f\"frame_{i}.jpg\")\n",
    "            cv2.imwrite(frame_filename, frame)\n",
    "            frame_list.append(frame_filename)\n",
    "        else:\n",
    "            print(f\"Failed to read frame at position {start_frame}\")\n",
    "    cap.release()\n",
    "    video_manager.release()  # Release the video manager resources\n",
    "    return frame_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory for frames\n",
    "output_dir = 'frames'  # Local directory to save frames\n",
    "\n",
    "# Extract key frames from the video\n",
    "frame_list = extract_key_frames(video_local_path, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading Images to S3\n",
    "We'll upload the extracted frames to S3 and get their public URLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to upload image to S3 and get public URL\n",
    "def upload_image_to_s3(image_path, s3_bucket, s3_folder='frames'):\n",
    "    file_name = os.path.basename(image_path)\n",
    "    s3_key = f\"{s3_folder}/{file_name}\"\n",
    "    s3.upload_file(image_path, s3_bucket, s3_key)\n",
    "    image_url = f\"https://{s3_bucket}.s3.{s3_region}.amazonaws.com/{s3_key}\"\n",
    "    return image_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Captions with OpenAI\n",
    "We'll generate captions for each frame using OpenAI's GPT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate captions using OpenAI's gpt-4o-mini model\n",
    "def generate_caption(image_url):\n",
    "    caption_system_prompt = '''\n",
    "    You are an assistant that generates concise captions for images. These captions will be embedded and stored so \n",
    "    people can semantically search for scenes. Ensure your captions include:\n",
    "    - Physical descriptions of people\n",
    "    - Identify and name of key actors\n",
    "    - Descriptions of key scene objects such as their color\n",
    "    - The mood of the scene\n",
    "    - The actions or activites taking place in the scene\n",
    "    '''\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": caption_system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": image_url,\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=300,\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Embeddings\n",
    "Compute embeddings for the captions using OpenAI's embedding models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get embeddings\n",
    "def get_embedding(value, model=\"text-embedding-3-large\"): \n",
    "    embeddings = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=value,\n",
    "        encoding_format=\"float\"\n",
    "    )\n",
    "    return embeddings.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching Frames Based on a Prompt\n",
    "We'll search for frames that are most similar to a user's query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search frames based on a prompt\n",
    "def search_frames(prompt, df_frames, top_n=5):\n",
    "    prompt_embedding = get_embedding(prompt)\n",
    "    df_frames['similarity'] = df_frames['embedding'].apply(\n",
    "        lambda x: cosine_similarity(np.array(x).reshape(1, -1), np.array(prompt_embedding).reshape(1, -1))[0][0]\n",
    "    )\n",
    "    results = df_frames.sort_values('similarity', ascending=False).head(top_n)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting It All Together\n",
    "We'll process each frame, generate captions and embeddings, and store the data in a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single frame\n",
    "def process_frame(frame_filename):\n",
    "    # Upload image to S3 and get the URL\n",
    "    image_url = upload_image_to_s3(frame_filename, s3_bucket_name)\n",
    "    # Generate caption\n",
    "    caption = generate_caption(image_url)\n",
    "    # Generate embedding\n",
    "    embedding = get_embedding(caption)\n",
    "    # Return frame data\n",
    "    return {\n",
    "        'frame_filename': frame_filename,\n",
    "        'image_url': image_url,\n",
    "        'caption': caption,\n",
    "        'embedding': embedding\n",
    "    }\n",
    "\n",
    "# Process frames in parallel\n",
    "frame_data_list = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    results = executor.map(process_frame, frame_list)\n",
    "    frame_data_list = list(results)\n",
    "\n",
    "# Create DataFrame\n",
    "df_frames = pd.DataFrame(frame_data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Query and Displaying Results\n",
    "Now, let's allow the user to search for scenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Find images of Paul Walker, a blonde hair blue-eyed man, in a car.\"\n",
    "results = search_frames(prompt, df_frames, top_n=5)\n",
    "\n",
    "# Display the results\n",
    "for index, row in results.iterrows():\n",
    "    print(f\"Frame: {row['frame_filename']}\")\n",
    "    print(f\"Caption: {row['caption']}\")\n",
    "    print(f\"Similarity: {row['similarity']}\")\n",
    "    display(Image(url=row['image_url']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
